import csv
import os
import subprocess
import tempfile
import shutil
from urllib.parse import urlparse
from dotenv import load_dotenv

load_dotenv()

GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
BASE_URL = os.getenv("BASE_URL", "https://github-test.qualcomm.com")
INPUT_CSV = "urlTestGreater.csv"
OUTPUT_CSV = "large_files_report.csv"
SIZE_THRESHOLD = 400 * 1024 * 1024  # 400MB in bytes


def parse_repo_url(url):
    parts = urlparse(url)
    path = parts.path.strip("/")
    if path.count("/") != 1:
        return None, None
    owner, repo = path.split("/")
    return owner, repo


def run_git_sizer(repo_path):
    try:
        result = subprocess.run(
            ["git", "sizer", "--verbose"],
            cwd=repo_path,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        output = result.stdout
        for line in output.splitlines():
            if "biggest blob" in line.lower():
                size_str = line.split()[-1].replace(",", "")
                if size_str.endswith("MB"):
                    size_val = float(size_str.replace("MB", ""))
                    return size_val > 400
        return False
    except Exception:
        return False


def deep_scan_repo(repo_path, repo_name):
    try:
        output = subprocess.run(["git", "rev-list", "--all"],
                                cwd=repo_path, stdout=subprocess.PIPE, text=True)
        commits = output.stdout.strip().splitlines()

        large_files = []
        for commit in commits:
            tree_cmd = ["git", "ls-tree", "-r", "-l", commit]
            tree_output = subprocess.run(tree_cmd, cwd=repo_path,
                                         stdout=subprocess.PIPE, text=True)
            for line in tree_output.stdout.strip().splitlines():
                parts = line.split()
                if len(parts) < 4:
                    continue
                try:
                    size = int(parts[-2]) if parts[-2].isdigit() else 0
                    path = parts[-1]
                    if size > SIZE_THRESHOLD:
                        large_files.append((repo_name, commit, path, size))
                except Exception:
                    continue
        return large_files
    except Exception as e:
        return [("ERROR", repo_name, "scan_failed", str(e))]


def scan_repo(url):
    owner, repo = parse_repo_url(url)
    if not owner or not repo:
        return [("ERROR", url, "invalid_repo_path", 0)]

    repo_url = f"https://{GITHUB_TOKEN}@{BASE_URL}/{owner}/{repo}.git"
    temp_dir = tempfile.mkdtemp()

    try:
        subprocess.run(["git", "clone", "--mirror", repo_url, temp_dir],
                       check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        sizer_flag = run_git_sizer(temp_dir)
        if not sizer_flag:
            return [(repo_url, "", "No blob over 400MB (git-sizer)", "")]

        return deep_scan_repo(temp_dir, repo_url)

    except Exception as e:
        return [(repo_url, "", "CLONE_FAILED", str(e))]
    finally:
        shutil.rmtree(temp_dir)


def main():
    with open(INPUT_CSV, newline="") as infile, open(OUTPUT_CSV, "w", newline="") as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        writer.writerow(["Repository", "Commit", "File Path", "Size (Bytes)"])

        for row in reader:
            url = row[0].strip()
            if not url or url.startswith("#"):
                continue
            results = scan_repo(url)
            for result in results:
                writer.writerow(result)


if __name__ == "__main__":
    main()
