import os
import csv
import subprocess
import logging
from dotenv import load_dotenv
import shutil

# Load environment variables
load_dotenv()

# Setup logging
LOG_FILE = "large_file_scan.log"
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def log_and_print(message):
    print(message)
    logging.info(message)

# Constants
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
INPUT_CSV = "urlTestGreater.csv"
OUTPUT_CSV = "large_files_report.csv"
CLONE_DIR = "cloned_repos"
SIZE_THRESHOLD = 419430400  # 400MB

# Ensure the clone directory exists
os.makedirs(CLONE_DIR, exist_ok=True)

# Prepare output CSV
output_rows = [["Repo URL", "Clone Status", "Scan Status", "Large Files (branch → path, size)", "Error"]]

# Read repo URLs
with open(INPUT_CSV, newline='') as infile:
    reader = csv.reader(infile)
    urls = [row[0].strip() for row in reader if row]

for url in urls:
    repo_name = url.split("/")[-1].replace(".git", "")
    clone_path = os.path.join(CLONE_DIR, repo_name)
    if os.path.exists(clone_path):
        shutil.rmtree(clone_path)

    authed_url = url.replace("https://", f"https://{GITHUB_TOKEN}@")

    clone_status = "Failed"
    scan_status = "Not Started"
    large_files = []
    error_message = ""

    try:
        subprocess.run(["git", "clone", "--mirror", authed_url, clone_path],
                       check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        log_and_print(f"CLONE SUCCESS: {url}")
        clone_status = "Success"

        try:
            commits = subprocess.check_output(["git", "rev-list", "--all"], cwd=clone_path).decode().splitlines()
            seen_blobs = set()

            for commit in commits:
                tree_output = subprocess.check_output(["git", "ls-tree", "-r", commit], cwd=clone_path).decode().splitlines()
                for line in tree_output:
                    parts = line.split()
                    if len(parts) < 4:
                        continue
                    blob_hash = parts[2]
                    file_path = parts[3]

                    if blob_hash in seen_blobs:
                        continue
                    seen_blobs.add(blob_hash)

                    try:
                        size = int(subprocess.check_output(["git", "cat-file", "-s", blob_hash], cwd=clone_path).decode().strip())
                        if size > SIZE_THRESHOLD:
                            branches = subprocess.check_output(["git", "branch", "--contains", commit], cwd=clone_path).decode().splitlines()
                            branch = branches[0].strip().replace("* ", "") if branches else "unknown"
                            large_files.append(f"{branch} → {file_path} ({size} bytes)")
                    except Exception:
                        continue

            scan_status = "Completed"
            if not large_files:
                large_files = ["None"]

            log_and_print(f"SCAN COMPLETE: {url} - Large files: {', '.join(large_files)}")

        except subprocess.CalledProcessError as e:
            scan_status = "Failed"
            error_message = e.stderr.decode()
            log_and_print(f"SCAN FAILED: {url} — {error_message}")

    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode()
        log_and_print(f"CLONE FAILED: {url} — {error_message}")

    output_rows.append([url, clone_status, scan_status, "; ".join(large_files), error_message])

    if os.path.exists(clone_path):
        shutil.rmtree(clone_path)

# Write results
with open(OUTPUT_CSV, "w", newline='') as outfile:
    writer = csv.writer(outfile)
    writer.writerows(output_rows)
