import os
import csv
import subprocess
import logging
from dotenv import load_dotenv
import shutil

# Load environment variables
load_dotenv()

# Setup logging
LOG_FILE = "large_file_scan.log"
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def log_and_print(message):
    print(message)
    logging.info(message)

# Constants
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
INPUT_CSV = "urlTestGreater.csv"
OUTPUT_CSV = "large_files_report.csv"
CLONE_DIR = "cloned_repos"
SIZE_THRESHOLD = 419430400  # 400MB

# Ensure the clone directory exists
os.makedirs(CLONE_DIR, exist_ok=True)

# Prepare output CSV
output_rows = [["Repo URL", "Clone Status", "Scan Status", "Large Files (path, size, SHA, URL)", "Error"]]

# Read repo URLs
with open(INPUT_CSV, newline='') as infile:
    reader = csv.reader(infile)
    urls = [row[0].strip() for row in reader if row]

for url in urls:
    repo_name = url.split("/")[-1].replace(".git", "")
    org_name = url.split("/")[-2]
    clone_path = os.path.join(CLONE_DIR, repo_name)
    if os.path.exists(clone_path):
        shutil.rmtree(clone_path)

    authed_url = url.replace("https://", f"https://{GITHUB_TOKEN}@")

    clone_status = "Failed"
    scan_status = "Not Started"
    large_files = []
    error_message = ""

    try:
        subprocess.run(["git", "clone", "--mirror", authed_url, clone_path],
                       check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        log_and_print(f"CLONE SUCCESS: {url}")
        clone_status = "Success"

        try:
            commits = subprocess.check_output(["git", "rev-list", "--all"], cwd=clone_path).decode().splitlines()
            seen_blobs = set()

            for commit in commits:
                tree_output = subprocess.check_output(["git", "ls-tree", "-r", commit], cwd=clone_path).decode().splitlines()
                for line in tree_output:
                    parts = line.split()
                    if len(parts) < 4:
                        continue
                    blob_hash = parts[2]
                    file_path = parts[3]

                    if blob_hash in seen_blobs:
                        continue
                    seen_blobs.add(blob_hash)

                    try:
                        size = int(subprocess.check_output(["git", "cat-file", "-s", blob_hash], cwd=clone_path).decode().strip())
                        if size > SIZE_THRESHOLD:
                            blob_url = f"https://github-test.qualcomm.com/{org_name}/{repo_name}/blob/{commit}/{file_path}"
                            large_files.append(f"{file_path} ({size} bytes, SHA: {blob_hash}, URL: {blob_url})")
                    except Exception:
                        continue

            scan_status = "Completed"
            if not large_files:
                large_files = ["None"]

            log_and_print(f"SCAN COMPLETE: {url} - Large files: {', '.join(large_files)}")

        except subprocess.CalledProcessError as e:
            scan_status = "Failed"
            error_message = e.stderr.decode()
            log_and_print(f"SCAN FAILED: {url} — {error_message}")

    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode()
        log_and_print(f"CLONE FAILED: {url} — {error_message}")

    output_rows.append([url, clone_status, scan_status, "; ".join(large_files), error_message])

# Write results
with open(OUTPUT_CSV, "w", newline='') as outfile:
    writer = csv.writer(outfile)
    writer.writerows(output_rows)
