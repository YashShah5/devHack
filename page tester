import os
import csv
import subprocess
import logging
import shutil
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Setup logging
LOG_FILE = "large_file_scan.log"
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def log_and_print(message):
    print(message)
    logging.info(message)

# Constants
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
INPUT_CSV = "urlTestGreater.csv"
OUTPUT_CSV = "enhanced_large_files_report.csv"
CLONE_DIR = "cloned_repos"
SIZE_THRESHOLD = 419430400  # 400MB

# Ensure clone directory exists
os.makedirs(CLONE_DIR, exist_ok=True)

# Prepare output CSV with headers
output_rows = [[
    "Repo URL", "Clone Status", "Scan Status", "File Path", "File Size (MB)",
    "Blob SHA", "Commit ID", "GitHub URL", "Branches Scanned", "Files Scanned",
    "Error", "Large File Detected"
]]

# Load repo URLs
with open(INPUT_CSV, newline='') as infile:
    reader = csv.reader(infile)
    urls = [row[0].strip() for row in reader if row]

# Process each repo
for url in urls:
    repo_name = url.split("/")[-1].replace(".git", "")
    owner = url.split("/")[-2]
    clone_path = os.path.join(CLONE_DIR, repo_name)

    # Clean up previous clone if it exists
    if os.path.exists(clone_path):
        shutil.rmtree(clone_path)

    authed_url = url.replace("https://", f"https://{GITHUB_TOKEN}@")

    clone_status = "Failed"
    scan_status = "Not Started"
    error_message = ""
    seen_blobs = set()
    total_files = 0
    total_branches = 0
    large_file_found = False

    try:
        subprocess.run(["git", "clone", "--mirror", authed_url, clone_path],
                       check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        log_and_print(f"CLONE SUCCESS: {url}")
        clone_status = "Success"

        branches = subprocess.check_output(
            ["git", "for-each-ref", "--format=%(refname:short)", "refs/heads/"],
            cwd=clone_path
        ).decode().splitlines()
        total_branches = len(branches)

        commits = subprocess.check_output(["git", "rev-list", "--all"], cwd=clone_path).decode().splitlines()

        for commit in commits:
            tree_output = subprocess.check_output(["git", "ls-tree", "-r", commit], cwd=clone_path).decode().splitlines()
            for line in tree_output:
                parts = line.split()
                if len(parts) < 4:
                    continue
                blob_hash = parts[2]
                file_path = parts[3]

                if blob_hash in seen_blobs:
                    continue
                seen_blobs.add(blob_hash)
                total_files += 1

                try:
                    size = int(subprocess.check_output(["git", "cat-file", "-s", blob_hash], cwd=clone_path).decode().strip())
                    if size > SIZE_THRESHOLD:
                        large_file_found = True
                        size_mb = round(size / (1024 * 1024), 2)
                        github_url = f"https://github.com/{owner}/{repo_name}/blob/{commit}/{file_path}"
                        output_rows.append([
                            url, clone_status, "Completed", file_path, size_mb,
                            blob_hash, commit, github_url, total_branches, total_files,
                            "", "Yes"
                        ])
                        log_and_print(f"LARGE FILE FOUND: {file_path} ({size_mb} MB) in {url}")
                except Exception:
                    continue

        scan_status = "Completed"

        # If no large files were found, still log the repo
        if not large_file_found:
            output_rows.append([
                url, clone_status, scan_status, "", "", "", "", "", total_branches,
                total_files, "", "No"
            ])

    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode()
        log_and_print(f"CLONE OR SCAN FAILED: {url} â€” {error_message}")
        output_rows.append([
            url, clone_status, scan_status, "", "", "", "", "", total_branches,
            total_files, error_message, "N/A"
        ])
        continue

# Write output CSV
with open(OUTPUT_CSV, "w", newline='') as outfile:
    writer = csv.writer(outfile)
    writer.writerows(output_rows)

log_and_print("Scan complete. Results written to enhanced_large_files_report.csv.")
